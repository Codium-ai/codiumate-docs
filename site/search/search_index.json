{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#solutions","title":"Solutions","text":"<ul> <li>PR-Agent - Create a new project.</li> <li>VSC Codiumate - Start the live-reloading docs server.</li> <li>JetBrains - Build the documentation site.</li> <li>AlphaCodium - Print help message and exit.</li> </ul>"},{"location":"#codeblocks","title":"Codeblocks","text":"<p>Some <code>code</code> goes here.</p>"},{"location":"#plain-codeblock","title":"Plain codeblock","text":"<p>A plain codeblock:</p> <pre><code>Some code here\ndef myfunction()\n// some comment\n</code></pre>"},{"location":"#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>py</code> at the start:</p> <pre><code>import tensorflow as tf\ndef whatever()\n</code></pre>"},{"location":"#with-a-title","title":"With a title","text":"bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"#with-line-numbers","title":"With line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"#highlighting-lines","title":"Highlighting lines","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"#icons-and-emojs","title":"Icons and Emojs","text":""},{"location":"alphacodium/","title":"AlphaCodium","text":"<p>Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</p> <p>Paper | Dataset</p> <p>Official Implementation</p> <p>Tal Ridnik, Dedy Kredo, Itamar Friedman</p> <p>CodiumAI</p>"},{"location":"alphacodium/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Abstract</li> <li>Installation</li> <li>How to run</li> <li>Technical Q&amp;A</li> <li>Broader Applicability</li> <li>Example Problem</li> <li>Acknowledgments</li> <li>Citation</li> </ul>"},{"location":"alphacodium/#abstract","title":"Abstract","text":"<p>Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks.</p> <p>In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.</p> <p>We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. </p> <p>Many of the principles and best practices we acquired in this work, we believe, are broadly applicable to general code generation tasks.</p> <p> </p>"},{"location":"alphacodium/#installation","title":"Installation","text":"<p>(1) setup a virtual environment and run: <code>pip install -r requirements.txt</code></p> <p>(2) Duplicate the file <code>alpha_codium/settings/.secrets_template.toml</code>, rename it as <code>.secrets.toml</code>, and fill in your OpenAI API key: <pre><code>[openai]\nkey = \"...\"\n</code></pre></p> <p>(3) Download the processed CodeContest validation and test dataset from hugging face, extract the zip file, and placed the extracted folder in the root of the project.</p>"},{"location":"alphacodium/#how-to-run","title":"How to run","text":""},{"location":"alphacodium/#configuration","title":"Configuration","text":"<p>The file: <code>alpha_codium/settings/configuration.toml</code> contains the configuration for the project. In the <code>config</code> section you can choose the model you want to use (\"gpt-4\", \"gpt-3.5-turbo-16k\", or others).</p>"},{"location":"alphacodium/#solving-a-specific-problem","title":"Solving a specific problem","text":"<p>To solve a specific problem with AlphaCodium, from the root folder run: <pre><code>python -m alpha_codium.solve_problem \\\n--dataset_name /path/to/dataset \\\n--split_name test \\\n--problem_number 0\n</code></pre> - The <code>dataset_name</code> is the path to the dataset folder you downloaded in the installation step. - Note that the validation set contains 117 problems, and the test set contains 165 problems, so the <code>problem_number</code> parameter should be accordingly (zero-based) - The <code>split_name</code> can be either <code>valid</code> or <code>test</code>. - The following sections in the configuration file:  <code>solve</code>, <code>self_reflection</code>,<code>possible_solutions</code>,<code>generate_ai_tests</code>,<code>initial_code_generation</code>,<code>public_tests</code>, <code>ai_tests</code> enable to adjust possible configurations for the different stages of the flow. - Each run logs the results to a file named <code>alpha_codium/example.log</code>. Reviewing the log file is a good way to understand what is going on in each stage of the flow.</p> <p>Example problem (test set, problem number 12):</p> <p></p>"},{"location":"alphacodium/#solving-the-entire-dataset","title":"Solving the entire dataset","text":"<p>to solve the entire dataset with AlphaCodium, from the root folder run: <pre><code>python -m alpha_codium.solve_dataset \\\n--dataset_name /path/to/dataset \\\n--split_name test\n--database_solution_path /path/to/output/dir/dataset_output.json\n</code></pre></p> <ul> <li>The <code>split_name</code> can be either <code>valid</code> or <code>test</code>.</li> <li><code>database_solution_path</code> is the path to the directory where the solutions will be saved.</li> <li>The <code>dataset</code> section in the configuration file contains the configuration for the running and evaluation of a dataset.</li> <li>Note that this is a long process, and it may take a few days to complete with large models (e.g. GPT-4) and several iterations per problem. </li> <li><code>dataset.num_iterations</code> defines the number of iterations for each problem (pass@K). For a large number of iterations, it is recommended to introduce some randomness and different options for each iteration to achieve top results.</li> </ul>"},{"location":"alphacodium/#running-the-evaluation","title":"Running the evaluation","text":"<p>Once you generate a solution for the entire dataset (valid or test), you can evaluate it by running: <pre><code>python -m alpha_codium.evaluate_dataset\\\n--dataset_name /path/to/dataset\\\n--split_name test\\\n--database_solution_path /path/to/output/dir/dataset_output.json\n</code></pre></p>"},{"location":"alphacodium/#technical-qa","title":"Technical Q&amp;A","text":"<p>Aggregating some technical questions we received about this project:</p> <p>Q: How much time did you spend on \"prompt engineering\" compared to \"flow engineering\"? A: Structured output almost completely eliminates the need for simple prompt engineering. We estimate that ~95% of the time we did more high-level design, reasoning, and injecting data at the correct places, ..., a.k.a. \"flow engineering\".</p> <p>Q: How do you know that there wasn't a data leakage? A: The test set of CodeContests dataset comprises problems published after September 2021, while the GPT-4 model variant we used (gpt-4-0613) has a data cutoff of September 2021. Hence, there is no data leakage for GPT4, on the test set. For other models like DeepSeek, we cannot be sure. However, note that our main result is a comparison of \"direct prompt\" vs. \"AlphaCodium flow\". Data leakage would help both approaches, so the relative improvement of AlphaCodium flow is still valid.</p> <p>Q: Is this project relevant only to specific programming languages? A: No. The proposed flow is language agnostic. We generated solutions in Python, but the flow can be applied to any language.</p> <p>Q: How did you manage the context window? A: We used models with a context window of 8192 tokens, and we did not encounter cases where it did not suffice. However, we clearly observed that as the context we used in practice grows larger (let's say, above 4000 tokens), the model starts to \"ignore\" some of the information in the context. Hence, there is a clear tradeoff: - Injecting the results of previous stages into the context, may help the model to generate better code. - However, it may also cause the model to ignore specific details and nuances from the problem description.</p> <p>Q: Is this work \"realistic\" in terms of the number of LLM calls? A: In comparison to AlphaCode, we do four orders of magnitude (!) fewer calls (per solution AlphaCodium does 15-20 calls). Yet we acknowledge that for some applications, this may still be too much, and more optimizations are needed. We however believe that many of the ideas and principles we acquired in this work are broadly applicable, even when the number of calls is further limited.</p> <p>Q: Why do you iterate only on the generated code, and not on the AI-generated tests? A: For code problems in CodeContests, the tests are a list of input-output pairs. Hence, you don't really learn anything new when you \"fix\" a test - you just change its output to the prediction of the generated code. Instead of fixing tests, we preferred to always try and fix the code, while using \"test anchors\". (see the paper for more details). However, for other code generation tasks, where the tests are more complex and contain runnable code, iterating on the tests, in addition to iterating on the generated code, may be beneficial.</p>"},{"location":"alphacodium/#broader-applicability","title":"Broader Applicability","text":"<p>While this work presents results on CodeContests dataset, we believe that it has a broader applicability.</p> <p>First and foremost, we feel that the proposed AlphaCodium flow, with reasonable adjustments, can be used as a more general framework for other code generation tasks.</p> <p>Secondly, many of the design concepts, principles, and tricks we acquired in this work are broadly applicable as-is to any general code generation tasks. For example: - YAML Structured output: asking the model to generate an output in YAML format, equivalent to a given Pydantic class - Semantic reasoning via bullet points analysis: Bullet points analysis encourages an in-depth understanding of the problem, and forces the model to divide the output into logical semantic sections, leading to improved results - LLMs do better when generating a modular code: when asking the model to: <code>divide the generated code into small sub-functions, with meaningful names and functionality</code>, we observe a better-produced code, with fewer bugs, and higher success rates for the iterative fixing stages. - Soft decisions with double validation: with a double validation process, we add an extra step where, given the generated output, the model is asked to re-generate the same output, but correct it if needed - Leave room for exploration: since the model can be wrong, it\u2019s better to avoid irreversible decisions, and leave room for exploration and code iterations with different possible solutions</p> <p>The list above is partial. See the paper for more details. The code provided in this repo can be used as a reference for better understanding the proposed concepts, and for applying them to other code generation tasks.</p>"},{"location":"alphacodium/#example-problem","title":"Example Problem","text":"<p>In this section, we present an example for a full problem from CodeContests dataset (test-set, problem 1), in order to demonstrate the complexity of the problems in the dataset, and the challenges they pose to LLMs.</p> <pre><code>problem name: '1575_B. Building an Amusement Park'\n\nproblem description:\nMr. Chanek lives in a city represented as a plane. He wants to build an amusement park in the shape of a circle of radius r. \nThe circle must touch the origin (point (0, 0)).\nThere are n bird habitats that can be a photo spot for the tourists in the park. The i-th bird habitat is at point p_i = (x_i, y_i). \n\nFind the minimum radius r of a park with at least k bird habitats inside. \n\nA point is considered to be inside the park if and only if the distance between p_i and the center of the park is less than or equal \nto the radius of the park.\nNote that the center and the radius of the park do not need to be integers.\n\nIn this problem, it is guaranteed that the given input always has a solution with r \u2264 2 \u22c5 10^5.\n\nInput\n\nThe first line contains two integers n and k (1 \u2264 n \u2264 10^5, 1 \u2264 k \u2264 n) \u2014 the number of bird habitats in the city and the number of bird \nhabitats required to be inside the park.\nThe i-th of the next n lines contains two integers x_i and y_i (0 \u2264 |x_i|, |y_i| \u2264 10^5) \u2014 the position of the i-th bird habitat.\n\nOutput\n\nOutput a single real number r denoting the minimum radius of a park with at least k bird habitats inside. It is guaranteed that the given \ninput always has a solution with r \u2264 2 \u22c5 10^5.\nYour answer is considered correct if its absolute or relative error does not exceed 10^{-4}.\nFormally, let your answer be a, and the jury's answer be b. Your answer is accepted if and only if \\frac{|a - b|}{max{(1, |b|)}} \u2264 10^{-4}.\n\nExamples\n\nInput\n\n8 4\n-3 1\n-4 4\n1 5\n2 2\n2 -2\n-2 -4\n-1 -1\n-6 0\n\nOutput\n\n3.1622776589\n\n\nInput\n\n1 1\n0 0\n\n\nOutput\n\n0.0000000000\n\nNote\n\nIn the first example, Mr. Chanek can put the center of the park at (-3, -1) with radius \u221a{10} \u2248 3.162. It can be proven this is the minimum r.\n</code></pre>"},{"location":"alphacodium/#acknowledgments","title":"Acknowledgments","text":"<p>Our process CodeContests dataset is based on the original CodeContests dataset. We removed the train set (which is not relevant to our work) and did some post-processing and cleaning to the validation and test sets.</p>"},{"location":"alphacodium/#citation","title":"Citation","text":"<pre><code>@misc{ridnik2024code,\n      title={Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering}, \n      author={Tal Ridnik and Dedy Kredo and Itamar Friedman},\n      year={2024},\n      eprint={2401.08500},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"codiumate/","title":"VSC Codiumate","text":"<p>CodiumAI analyzes your code and generates meaningful tests to catch bugs before you ship. With CodiumAI, you can easily and quickly create comprehensive test suites that help you ensure the reliability and correctness of your software. It supports all languages!</p>"},{"location":"codiumate/documentation/configurationfile/","title":"Configuration File","text":"<p>TODO</p>"},{"location":"codiumate/documentation/extendtestsuite/","title":"Extend Test Suite","text":"<p>TODO</p>"},{"location":"codiumate/documentation/newtest/","title":"Generate Tests","text":"<p>TODO</p>"},{"location":"codiumate/documentation/codeassistant/ask/","title":"/ask","text":"<p>TODO</p>"},{"location":"codiumate/documentation/codeassistant/docstring/","title":"/docstring","text":"<p>TODO</p>"},{"location":"codiumate/documentation/codeassistant/enhance/","title":"/enhance","text":"<p>TODO</p>"},{"location":"codiumate/documentation/codeassistant/explain/","title":"/explain","text":"<p>TODO</p>"},{"location":"codiumate/documentation/codeassistant/improve/","title":"/improve","text":"<p>TODO</p>"},{"location":"codiumate/documentation/codeassistant/test/","title":"/test","text":"<p>TODO</p>"},{"location":"codiumate/documentation/codeassistant/threads/","title":"Threads","text":"<p>TODO</p>"},{"location":"codiumate/documentation/prassistant/analyze/","title":"/analyze","text":"<p>TODO</p>"},{"location":"codiumate/documentation/prassistant/ask/","title":"/ask","text":"<p>TODO</p>"},{"location":"codiumate/documentation/prassistant/changelog/","title":"/changelog","text":"<p>TODO</p>"},{"location":"codiumate/documentation/prassistant/commit/","title":"/commit","text":"<p>TODO</p>"},{"location":"codiumate/documentation/prassistant/describe/","title":"/describe","text":"<p>TODO</p>"},{"location":"codiumate/documentation/prassistant/docstring/","title":"/docstring","text":"<p>TODO</p>"},{"location":"codiumate/documentation/prassistant/improve/","title":"/improve","text":"<p>TODO</p>"},{"location":"codiumate/documentation/prassistant/review/","title":"/review","text":"<p>TODO</p>"},{"location":"codiumate/gettingstarted/authentication/","title":"Authentication","text":"<p>TODO</p>"},{"location":"codiumate/gettingstarted/configuration/","title":"Configuration","text":"<p>TODO</p>"},{"location":"codiumate/gettingstarted/configuration/#download-extensions","title":"Download extensions","text":""},{"location":"codiumate/gettingstarted/installation/","title":"Installation","text":"<p>TODO</p>"},{"location":"codiumate/guides/behaviorfoldout/","title":"Behavior Coverage in the advanced panel","text":"<p>TODO</p>"},{"location":"codiumate/guides/usereferencetests/","title":"Work with reference tests","text":"<p>TODO</p>"},{"location":"codiumate/guides/workwiththreads/","title":"Chat with CodiumAI Threads","text":"<p>TODO</p>"},{"location":"jebrains/","title":"JetBrains","text":"<p>CodiumAI analyzes your code and generates meaningful tests to catch bugs before you ship. With CodiumAI, you can easily and quickly create comprehensive test suites that help you ensure the reliability and correctness of your software. It supports all languages!</p>"},{"location":"jebrains/documentation/configurationfile/","title":"Configuration File","text":"<p>TODO</p>"},{"location":"jebrains/documentation/extendtestsuite/","title":"Extend Test Suite","text":"<p>TODO</p>"},{"location":"jebrains/documentation/newtest/","title":"Generate Tests","text":"<p>TODO</p>"},{"location":"jebrains/documentation/prassistant/analyze/","title":"/analyze","text":"<p>TODO</p>"},{"location":"jebrains/documentation/prassistant/ask/","title":"/ask","text":"<p>TODO</p>"},{"location":"jebrains/documentation/prassistant/changelog/","title":"/changelog","text":"<p>TODO</p>"},{"location":"jebrains/documentation/prassistant/commit/","title":"/commit","text":"<p>TODO</p>"},{"location":"jebrains/documentation/prassistant/describe/","title":"/describe","text":"<p>TODO</p>"},{"location":"jebrains/documentation/prassistant/docstring/","title":"/docstring","text":"<p>TODO</p>"},{"location":"jebrains/documentation/prassistant/improve/","title":"/improve","text":"<p>TODO</p>"},{"location":"jebrains/documentation/prassistant/review/","title":"/review","text":"<p>TODO</p>"},{"location":"jebrains/gettingstarted/authentication/","title":"Authentication","text":"<p>TODO</p>"},{"location":"jebrains/gettingstarted/configuration/","title":"Configuration","text":"<p>TODO</p>"},{"location":"jebrains/gettingstarted/configuration/#download-extensions","title":"Download extensions","text":""},{"location":"jebrains/gettingstarted/installation/","title":"Installation","text":"<p>TODO</p>"},{"location":"jebrains/guides/behaviorfoldout/","title":"Behavior Coverage in the advanced panel","text":"<p>TODO</p>"},{"location":"jebrains/guides/usereferencetests/","title":"Work with reference tests","text":"<p>TODO</p>"},{"location":"jebrains/guides/workwiththreads/","title":"Chat with CodiumAI Threads","text":"<p>TODO</p>"},{"location":"pragent/","title":"PR-Agent","text":""},{"location":"pragent/#table-of-contents","title":"Table of Contents","text":"<ul> <li>News and Updates</li> <li>Overview</li> <li>Example results</li> <li>Features overview</li> <li>Try it now</li> <li>Installation</li> <li>PR-Agent Pro \ud83d\udc8e</li> <li>How it works</li> <li>Why use PR-Agent?</li> </ul>"},{"location":"pragent/#news-and-updates","title":"News and Updates","text":""},{"location":"pragent/#jan-21-2024","title":"Jan 21, 2024","text":"<ul> <li>\ud83d\udc8e Custom suggestions - A new tool, <code>/custom_suggestions</code>, was added to PR-Agent Pro. The tool will propose only suggestions that follow specific guidelines defined by the user.  See here for more details.</li> </ul>"},{"location":"pragent/#jan-17-2024","title":"Jan 17, 2024","text":"<ul> <li>\ud83d\udc8e Inline file summary - The <code>describe</code> tool has a new option, <code>--pr_description.inline_file_summary</code>, which allows adding a summary of each file change to the Diffview page. See here</li> <li>The <code>improve</code> tool now can present suggestions in a nice collapsible format, which significantly reduces the PR footprint. See here for more details. </li> <li>To accompany the improved interface of the  <code>improve</code> tool, we change the default automation settings of our GithupApp to: <pre><code>pr_commands = [\n    \"/describe --pr_description.add_original_user_description=true --pr_description.keep_original_user_title=true\",\n    \"/review --pr_reviewer.num_code_suggestions=0\",\n    \"/improve --pr_code_suggestions.summarize=true\",\n]\n</code></pre> Meaning that by default, for each PR the <code>describe</code>, <code>review</code>, and <code>improve</code> tools will be triggered automatically, and the <code>improve</code> tool will present the suggestions in a single comment. You can of course overwrite these defaults by adding a <code>.pr_agent.toml</code> file to your repo. See here.</li> </ul>"},{"location":"pragent/#overview","title":"Overview","text":"<p>CodiumAI PR-Agent is an open-source tool to help efficiently review and handle pull requests. It automatically analyzes the pull request and can provide several types of commands:</p> <p>\u2023 Auto Description (<code>/describe</code>): Automatically generating PR description - title, type, summary, code walkthrough and labels. \\ \u2023 Auto Review (<code>/review</code>): Adjustable feedback about the PR main theme, type, relevant tests, security issues, score, and various suggestions for the PR content. \\ \u2023 Question Answering (<code>/ask ...</code>): Answering free-text questions about the PR. \\ \u2023 Code Suggestions (<code>/improve</code>): Committable code suggestions for improving the PR. \\ \u2023 Update Changelog (<code>/update_changelog</code>): Automatically updating the CHANGELOG.md file with the PR changes. \\ \u2023 Find Similar Issue (<code>/similar_issue</code>): Automatically retrieves and presents similar issues. \\ \u2023 Add Documentation \ud83d\udc8e  (<code>/add_docs</code>): Automatically adds documentation to methods/functions/classes that changed in the PR. \\ \u2023 Generate Custom Labels \ud83d\udc8e (<code>/generate_labels</code>): Automatically suggests custom labels based on the PR code changes. \\ \u2023 Analyze \ud83d\udc8e (<code>/analyze</code>): Automatically analyzes the PR, and presents changes walkthrough for each component. \\ \u2023 Custom Suggestions \ud83d\udc8e (<code>/custom_suggestions</code>): Automatically generates custom suggestions for improving the PR code, based on specific guidelines defined by the user.</p> <p>See the Installation Guide for instructions on installing and running the tool on different git platforms.</p> <p>See the Usage Guide for running the PR-Agent commands via different interfaces, including CLI, online usage, or by automatically triggering them when a new PR is opened.</p> <p>See the Tools Guide for a detailed description of the different tools (tools are run via the commands).</p>"},{"location":"pragent/#example-results","title":"Example results","text":"<p>/describe</p> <p></p> <p>/improve</p> <p></p> <p>[/generate_labels]https://github.com/Codium-ai/pr-agent/pull/530)</p> <p></p> <p>/reflect_and_review</p> <p></p> <p>/ask</p> <p></p> <p>/improve</p> <p></p>"},{"location":"pragent/#features-overview","title":"Features overview","text":"<p><code>PR-Agent</code> offers extensive pull request functionalities across various git providers: |       |                                             | GitHub | Gitlab | Bitbucket | |-------|---------------------------------------------|:------:|:------:|:---------:| | TOOLS | Review                                      |       |       |          | |       | \u2b91 Incremental                              |       |                         |                            | |       | \u2b91 SOC2 Compliance \ud83d\udc8e                       |       |       |           | |       | Ask                                         |       |       |           | |       | Describe                                    |       |       |           | |       | \u2b91 Inline file summary \ud83d\udc8e                       |       |       |           | |       | Improve                                     |       |       |           | |       | \u2b91 Extended                                 |       |       |           | |       | Custom Suggestions \ud83d\udc8e        |       |       |           | |       | Reflect and Review                          |       |       |           | |       | Update CHANGELOG.md                         |       |       |           | |       | Find Similar Issue                          |       |                         |                             | |       | Add PR Documentation \ud83d\udc8e                     |       |       |           | |       | Generate Custom Labels \ud83d\udc8e                   |       |       |         | |       | Analyze PR Components \ud83d\udc8e                    |       |       |         | |       |                                             |        |        |      | | USAGE | CLI                                         |       |       |          | |       | App / webhook                               |       |       |           | |       | Tagging bot                                 |       |        |           |  |       | Actions                                     |       |        |           |  |       |                                             |        |        |      | | CORE  | PR compression                              |       |       |          | |       | Repo language prioritization                |       |       |          | |       | Adaptive and token-aware file patch fitting |       |       |        | |       | Multiple models support |       |       |          |  | |       | Incremental PR review |       |      |      | |       | Static code analysis \ud83d\udc8e |       |        |        | |       | Global configuration \ud83d\udc8e |       |        |        |</p> <ul> <li>\ud83d\udc8e means this feature is available only in PR-Agent Pro</li> <li>Support for additional git providers is described in here </li> </ul>"},{"location":"pragent/#try-it-now","title":"Try it now","text":"<p>Try the GPT-4 powered PR-Agent instantly on your public GitHub repository. Just mention <code>@CodiumAI-Agent</code> and add the desired command in any PR comment. The agent will generate a response based on your command. For example, add a comment to any pull request with the following text: <pre><code>@CodiumAI-Agent /review\n</code></pre> and the agent will respond with a review of your PR</p> <p></p> <p>To set up your own PR-Agent, see the Installation section below. Note that when you set your own PR-Agent or use CodiumAI hosted PR-Agent, there is no need to mention <code>@CodiumAI-Agent ...</code>. Instead, directly start with the command, e.g., <code>/ask ...</code>.</p>"},{"location":"pragent/#installation","title":"Installation","text":"<p>To use your own version of PR-Agent, you first need to acquire two tokens:</p> <ol> <li>An OpenAI key from here, with access to GPT-4.</li> <li>A GitHub personal access token (classic) with the repo scope.</li> </ol> <p>There are several ways to use PR-Agent:</p> <ul> <li>Method 1: Use Docker image (no installation required)</li> <li>Method 2: Run from source</li> <li>Method 3: Run as a GitHub Action</li> <li>Method 4: Run as a polling server</li> <li>Request reviews by tagging your GitHub user on a PR</li> <li>Method 5: Run as a GitHub App</li> <li>Allowing you to automate the review process on your private or public repositories</li> <li>Method 6: Deploy as a Lambda Function</li> <li>Method 7: AWS CodeCommit</li> <li>Method 8: Run a GitLab webhook server</li> <li>Method 9: Run as a Bitbucket Pipeline</li> </ul>"},{"location":"pragent/#pr-agent-pro","title":"PR-Agent Pro \ud83d\udc8e","text":"<p>PR-Agent Pro is a hosted version of PR-Agent, provided by CodiumAI. It is available for a monthly fee, and provides the following benefits: 1. Fully managed - We take care of everything for you - hosting, models, regular updates, and more. Installation is as simple as signing up and adding the PR-Agent app to your GitHub\\BitBucket repo. 2. Improved privacy - No data will be stored or used to train models. PR-Agent Pro will employ zero data retention, and will use an OpenAI account with zero data retention. 3. Improved support - PR-Agent Pro users will receive priority support, and will be able to request new features and capabilities. 4. Extra features -In addition to the benefits listed above, PR-Agent Pro will emphasize more customization, and the usage of static code analysis, in addition to LLM logic, to improve results. It has the following additional features:     - SOC2 compliance check     - PR documentation     - Custom labels     - Global configuration     - Analyze PR components     - Custom Code Suggestions [WIP]     - Chat on Specific Code Lines [WIP]</p>"},{"location":"pragent/#how-it-works","title":"How it works","text":"<p>The following diagram illustrates PR-Agent tools and their flow:</p> <p></p> <p>Check out the PR Compression strategy page for more details on how we convert a code diff to a manageable LLM prompt</p>"},{"location":"pragent/#why-use-pr-agent","title":"Why use PR-Agent?","text":"<p>A reasonable question that can be asked is: <code>\"Why use PR-Agent? What makes it stand out from existing tools?\"</code></p> <p>Here are some advantages of PR-Agent:</p> <ul> <li>We emphasize real-life practical usage. Each tool (review, improve, ask, ...) has a single GPT-4 call, no more. We feel that this is critical for realistic team usage - obtaining an answer quickly (~30 seconds) and affordably.</li> <li>Our PR Compression strategy  is a core ability that enables to effectively tackle both short and long PRs.</li> <li>Our JSON prompting strategy enables to have modular, customizable tools. For example, the '/review' tool categories can be controlled via the configuration file. Adding additional categories is easy and accessible.</li> <li>We support multiple git providers (GitHub, Gitlab, Bitbucket), multiple ways to use the tool (CLI, GitHub Action, GitHub App, Docker, ...), and multiple models (GPT-4, GPT-3.5, Anthropic, Cohere, Llama2).</li> </ul>"},{"location":"pragent/#data-privacy","title":"Data privacy","text":"<p>If you host PR-Agent with your OpenAI API key, it is between you and OpenAI. You can read their API data privacy policy here: https://openai.com/enterprise-privacy</p> <p>When using PR-Agent Pro \ud83d\udc8e, hosted by CodiumAI, we will not store any of your data, nor will we use it for training. You will also benefit from an OpenAI account with zero data retention.</p>"},{"location":"pragent/#links","title":"Links","text":"<ul> <li>Discord community: https://discord.gg/kG35uSHDBc</li> <li>CodiumAI site: https://codium.ai</li> <li>Blog: https://www.codium.ai/blog/</li> <li>Troubleshooting: https://www.codium.ai/blog/technical-faq-and-troubleshooting/</li> <li>Support: support@codium.ai</li> </ul>"},{"location":"pragent/documentation/ADD_DOCUMENTATION/","title":"Add Documentation Tool \ud83d\udc8e","text":"<p>The <code>add_docs</code> tool scans the PR code changes, and automatically suggests documentation for any code components that changed in the PR (functions, classes, etc.).</p> <p>It can be invoked manually by commenting on any PR: <pre><code>/add_docs\n</code></pre> For example:</p> <p></p> <p></p> <p></p>"},{"location":"pragent/documentation/ADD_DOCUMENTATION/#configuration-options","title":"Configuration options","text":"<ul> <li><code>docs_style</code>: The exact style of the documentation (for python docstring). you can choose between: <code>google</code>, <code>numpy</code>, <code>sphinx</code>, <code>restructuredtext</code>, <code>plain</code>. Default is <code>sphinx</code>.</li> <li><code>extra_instructions</code>: Optional extra instructions to the tool. For example: \"focus on the changes in the file X. Ignore change in ...\".</li> </ul> <p>Notes  - Language that are currently fully supported: Python, Java, C++, JavaScript, TypeScript. - For languages that are not fully supported, the tool will suggest documentation only for new components in the PR. - A previous version of the tool, that offered support only for new components, was deprecated.</p>"},{"location":"pragent/documentation/ASK/","title":"ASK Tool","text":"<p>The <code>ask</code> tool answers questions about the PR, based on the PR code changes. Make sure to be specific and clear in your questions. It can be invoked manually by commenting on any PR: <pre><code>/ask \"...\"\n</code></pre> For example:</p> <p></p> <p></p> <p>Note that the tool does not have \"memory\" of previous questions, and answers each question independently.</p>"},{"location":"pragent/documentation/Analyze/","title":"Analyze Tool \ud83d\udc8e","text":"<p>The <code>analyze</code> tool combines static code analysis with LLM capabilities to provide a comprehensive analysis of the PR code changes.</p> <p>The tool scans the PR code changes, find the code components (methods, functions, classes) that changed, and summarizes the changes in each component.</p> <p>It can be invoked manually by commenting on any PR: <pre><code>/analyze\n</code></pre></p> <p>An example result:</p> <p></p> <p></p> <p></p> <p>Notes  - Language that are currently supported: Python, Java, C++, JavaScript, TypeScript.</p>"},{"location":"pragent/documentation/CUSTOM_SUGGESTIONS/","title":"Custom Suggestions Tool \ud83d\udc8e","text":""},{"location":"pragent/documentation/CUSTOM_SUGGESTIONS/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Example usage</li> <li>Configuration options</li> </ul>"},{"location":"pragent/documentation/CUSTOM_SUGGESTIONS/#overview","title":"Overview","text":"<p>The <code>custom_suggestions</code> tool scans the PR code changes, and automatically generates custom suggestions for improving the PR code. It shares similarities with the <code>improve</code> tool, but with one main difference: the <code>custom_suggestions</code> tool will only propose suggestions that follow specific guidelines defined by the prompt in: <code>pr_custom_suggestions.prompt</code> configuration.</p> <p>The tool can be triggered automatically every time a new PR is opened, or can be invoked manually by commenting on a PR.</p> <p>When commenting, use the following template:</p> <pre><code>/custom_suggestions --pr_custom_suggestions.prompt=\"The suggestions should focus only on the following:\\n-...\\n-...\\n-...\"\n</code></pre> <p>With a configuration file, use the following template:</p> <p><pre><code>[pr_custom_suggestions]\nprompt=\"\"\"\\\nThe suggestions should focus only on the following:\n-...\n-...\n-...\n\"\"\"\n</code></pre> Using a configuration file is recommended, since it allows to use multi-line instructions.</p> <p>Don't forget - with this tool, you are the prompter. Be specific, clear, and concise in the instructions. Specify relevant aspects that you want the model to focus on. \\ You might benefit from several trial-and-error iterations, until you get the correct prompt for your use case.</p>"},{"location":"pragent/documentation/CUSTOM_SUGGESTIONS/#example-usage","title":"Example usage","text":"<p>Here is an example of a possible prompt: <pre><code>[pr_custom_suggestions]\nprompt=\"\"\"\\\nThe suggestions should focus only on the following:\n- look for edge cases when implementing a new function\n- make sure every variable has a meaningful name\n- make sure the code is efficient\n\"\"\"\n</code></pre></p> <p>The instructions above are just an example. We want to emphasize that the prompt should be specific and clear, and be tailored to the needs of your project.</p> <p>Results obtained with the prompt above:</p> <p></p> <p></p>"},{"location":"pragent/documentation/CUSTOM_SUGGESTIONS/#configuration-options","title":"Configuration options","text":"<p><code>prompt</code>: the prompt for the tool. It should be a multi-line string.</p> <p><code>num_code_suggestions</code>: number of code suggestions provided by the 'custom_suggestions' tool. Default is 4.</p> <p><code>enable_help_text</code>: if set to true, the tool will display a help text in the comment. Default is true.</p>"},{"location":"pragent/documentation/DESCRIBE/","title":"Describe Tool","text":""},{"location":"pragent/documentation/DESCRIBE/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Configuration options</li> <li>Inline file summary \ud83d\udc8e</li> <li>Handle custom labels from the Repo's labels page </li> <li>Markers template</li> <li>Usage Tips</li> <li>Automation</li> <li>Custom labels</li> </ul>"},{"location":"pragent/documentation/DESCRIBE/#overview","title":"Overview","text":"<p>The <code>describe</code> tool scans the PR code changes, and generates a description for the PR - title, type, summary, walkthrough and labels.</p> <p>The tool can be triggered automatically every time a new PR is opened, or it can be invoked manually by commenting on any PR: <pre><code>/describe\n</code></pre> For example:</p> <p></p> <p></p>"},{"location":"pragent/documentation/DESCRIBE/#configuration-options","title":"Configuration options","text":"<p>To edit configurations related to the describe tool (<code>pr_description</code> section), use the following template: <pre><code>/describe --pr_description.some_config1=... --pr_description.some_config2=...\n</code></pre></p> <p>Possible configurations: - <code>publish_labels</code>: if set to true, the tool will publish the labels to the PR. Default is true.</p> <ul> <li> <p><code>publish_description_as_comment</code>: if set to true, the tool will publish the description as a comment to the PR. If false, it will overwrite the origianl description. Default is false.</p> </li> <li> <p><code>add_original_user_description</code>: if set to true, the tool will add the original user description to the generated description. Default is false.</p> </li> <li> <p><code>keep_original_user_title</code>: if set to true, the tool will keep the original PR title, and won't change it. Default is false.</p> </li> <li> <p><code>extra_instructions</code>: Optional extra instructions to the tool. For example: \"focus on the changes in the file X. Ignore change in ...\".</p> </li> <li> <p>To enable <code>custom labels</code>, apply the configuration changes described here</p> </li> <li> <p><code>enable_pr_type</code>: if set to false, it will not show the <code>PR type</code> as a text value in the description content. Default is true.</p> </li> <li> <p><code>final_update_message</code>: if set to true, it will add a comment message <code>PR Description updated to latest commit...</code> after finishing calling <code>/describe</code>. Default is true.</p> </li> <li> <p><code>enable_semantic_files_types</code>: if set to true, \"Changes walkthrough\" section will be generated. Default is true.</p> </li> <li><code>collapsible_file_list</code>: if set to true, the file list in the \"Changes walkthrough\" section will be collapsible. If set to \"adaptive\", the file list will be collapsible only if there are more than 8 files. Default is \"adaptive\".</li> </ul>"},{"location":"pragent/documentation/DESCRIBE/#inline-file-summary","title":"Inline file summary \ud83d\udc8e","text":"<p>This feature is available only in PR-Agent Pro</p> <p>This feature will enable you to quickly understand the changes in each file while reviewing the code changes (diff view).</p> <p>To add the walkthrough table to the \"Files changed\" tab, you can click on the checkbox that appears PR Description status message below the main PR Description:</p> <p></p> <p>If you prefer to have the file summaries appear in the \"Files changed\" tab on every PR, change the <code>pr_description.inline_file_summary</code> parameter in the configuration file, possible values are:</p> <ul> <li> <p><code>'table'</code>: File changes walkthrough table will be displayed on the top of the \"Files changed\" tab, in addition to the \"Conversation\" tab. </p> </li> <li> <p><code>true</code>: A collapsable file comment with changes title and a changes summary for each file in the PR. </p> </li> <li> <p><code>false</code> (<code>default</code>): File changes walkthrough will be added only to the \"Conversation\" tab.</p> </li> </ul> <p>Note that this feature is currently available only for GitHub.</p>"},{"location":"pragent/documentation/DESCRIBE/#handle-custom-labels-from-the-repos-labels-page","title":"Handle custom labels from the Repo's labels page","text":"<p>This feature is available only in PR-Agent Pro </p> <p>You can control  the custom labels that will be suggested by the <code>describe</code> tool, from the repo's labels page:</p> <ul> <li>GitHub : go to <code>https://github.com/{owner}/{repo}/labels</code> (or click on the \"Labels\" tab in the issues or PRs page)</li> <li>GitLab : go to <code>https://gitlab.com/{owner}/{repo}/-/labels</code> (or click on \"Manage\" -&gt; \"Labels\" on the left menu)</li> </ul> <p>Now add/edit the custom labels. they should be formatted as follows: * Label name: The name of the custom label. * Description: Start the description of with prefix <code>pr_agent:</code>, for example: <code>pr_agent: Description of when AI should suggest this label</code>.</p> <p>The description should be comprehensive and detailed, indicating when to add the desired label. For example: </p>"},{"location":"pragent/documentation/DESCRIBE/#markers-template","title":"Markers template","text":"<p>To enable markers, set <code>pr_description.use_description_markers=true</code>. Markers enable to easily integrate user's content and auto-generated content, with a template-like mechanism.</p> <p>For example, if the PR original description was: <pre><code>User content...\n\n## PR Type:\npr_agent:type\n\n## PR Description:\npr_agent:summary\n\n## PR Walkthrough:\npr_agent:walkthrough\n</code></pre> The marker <code>pr_agent:type</code> will be replaced with the PR type, <code>pr_agent:summary</code> will be replaced with the PR summary, and <code>pr_agent:walkthrough</code> will be replaced with the PR walkthrough.</p> <p></p> <p>==&gt;</p> <p></p> <p>Configuration params:</p> <ul> <li><code>use_description_markers</code>: if set to true, the tool will use markers template. It replaces every marker of the form <code>pr_agent:marker_name</code> with the relevant content. Default is false.</li> <li><code>include_generated_by_header</code>: if set to true, the tool will add a dedicated header: 'Generated by PR Agent at ...' to any automatic content. Default is true.</li> </ul>"},{"location":"pragent/documentation/DESCRIBE/#usage-tips","title":"Usage Tips","text":"<p>1)  Automation 2) Custom labels</p>"},{"location":"pragent/documentation/DESCRIBE/#automation","title":"Automation","text":"<ul> <li>When you first install the app, the default mode for the describe tool is: <pre><code>pr_commands = [\"/describe --pr_description.add_original_user_description=true\" \n                         \"--pr_description.keep_original_user_title=true\", ...]\n</code></pre> meaning the <code>describe</code> tool will run automatically on every PR, will keep the original title, and will add the original user description above the generated description.   This default settings aim to strike a good balance between automation and control: If you want more automation, just give the PR a title, and the tool will auto-write a full description; If you want more control, you can add a detailed description, and the tool will add the complementary description below it.</li> <li>For maximal automation, you can change the default mode to: <pre><code>pr_commands = [\"/describe --pr_description.add_original_user_description=false\" \n                         \"--pr_description.keep_original_user_title=true\", ...]\n</code></pre> so the title will be auto-generated as well.</li> <li>Markers are an alternative way to control the generated description, to give maximal control to the user. If you set: <pre><code>pr_commands = [\"/describe --pr_description.use_description_markers=true\", ...]\n</code></pre> the tool will replace every marker of the form <code>pr_agent:marker_name</code> in the PR description with the relevant content, where <code>marker_name</code> is one of the following:</li> <li><code>type</code>: the PR type.</li> <li><code>summary</code>: the PR summary.</li> <li><code>walkthrough</code>: the PR walkthrough.</li> </ul> <p>Note that when markers are enabled, if the original PR description does not contain any markers, the tool will not alter the description at all.</p>"},{"location":"pragent/documentation/DESCRIBE/#custom-labels","title":"Custom labels","text":"<p>The default labels of the describe tool are quite generic, since they are meant to be used in any repo: [<code>Bug fix</code>, <code>Tests</code>, <code>Enhancement</code>, <code>Documentation</code>, <code>Other</code>].</p> <p>If you specify custom labels in the repo's labels page, you can get tailored labels for your use cases. Examples for custom labels: - <code>Main topic:performance</code> -  pr_agent:The main topic of this PR is performance - <code>New endpoint</code> -  pr_agent:A new endpoint was added in this PR - <code>SQL query</code> -  pr_agent:A new SQL query was added in this PR - <code>Dockerfile changes</code> - pr_agent:The PR contains changes in the Dockerfile - ...</p> <p>The list above is eclectic, and aims to give an idea of different possibilities. Define custom labels that are relevant for your repo and use cases. Note that Labels are not mutually exclusive, so you can add multiple label categories. Make sure to provide proper title, and a detailed and well-phrased description for each label, so the tool will know when to suggest it.</p>"},{"location":"pragent/documentation/Full_environments/","title":"Full environments","text":""},{"location":"pragent/documentation/Full_environments/#overview","title":"Overview","text":"<p><code>PR-Agent</code> offers extensive pull request functionalities across various git providers: |       |                                             | GitHub | Gitlab | Bitbucket | CodeCommit | Azure DevOps | Gerrit | |-------|---------------------------------------------|:------:|:------:|:---------:|:----------:|:----------:|:----------:| | TOOLS | Review                                      |       |       |          |       |       |       | |       | \u2b91 Incremental                              |       |       |           |       |          |     | |       | Ask                                         |       |       |             |             |  |      | |       | Auto-Description                            |       |       |           |       |       |     | |       | Improve Code                                |       |       |           |       |          |        | |       | \u2b91 Extended                             |       |       |           |       |          |     | |       | Reflect and Review                          |       |       |           |          |       |        | |       | Update CHANGELOG.md                         |       |       |           |       |          |       | |       | Find similar issue                          |       |                         |                             |          |          |       | |       | Add Documentation                           |       |       |           |       |          |        | |       | Generate Custom Labels \ud83d\udc8e                   |       |       |         |     |          |      | |       |                                             |        |        |      |      |      | | USAGE | CLI                                         |       |       |          |       |       | |       | App / webhook                               |       |       |           |          |          | |       | Tagging bot                                 |       |        |           |          |          | |       | Actions                                     |       |        |           |          |          | |       | Web server                                  |       |        |           |          |          |     | |       |                                             |        |        |      |      |      | | CORE  | PR compression                              |       |       |          |  |          |        | |       | Repo language prioritization                |       |       |          |  |          |        | |       | Adaptive and token-awarefile patch fitting |       |       |          |  |          |        | |       | Multiple models support |       |       |          |  |          |        | |       | Incremental PR Review |       |      |      |      |      |      |</p>"},{"location":"pragent/documentation/GENERATE_CUSTOM_LABELS/","title":"Generate Custom Labels \ud83d\udc8e","text":"<p>The <code>generate_labels</code> tool scans the PR code changes, and given a list of labels and their descriptions, it automatically suggests labels that match the PR code changes.</p> <p>It can be invoked manually by commenting on any PR: <pre><code>/generate_labels\n</code></pre> For example:</p> <p>If we wish to add detect changes to SQL queries in a given PR, we can add the following custom label along with its description:</p> <p></p> <p>When running the <code>generate_labels</code> tool on a PR that includes changes in SQL queries, it will automatically suggest the custom label: </p> <p>Note that in addition to the dedicated tool <code>generate_labels</code>, the custom labels will also be used by the <code>describe</code> tool.</p>"},{"location":"pragent/documentation/GENERATE_CUSTOM_LABELS/#how-to-enable-custom-labels","title":"How to enable custom labels","text":"<p>There are 3 ways to enable custom labels:</p>"},{"location":"pragent/documentation/GENERATE_CUSTOM_LABELS/#1-cli-local-configuration-file","title":"1. CLI (local configuration file)","text":"<p>When working from CLI, you need to apply the configuration changes to the custom_labels file:</p>"},{"location":"pragent/documentation/GENERATE_CUSTOM_LABELS/#2-repo-configuration-file","title":"2. Repo configuration file","text":"<p>To enable custom labels, you need to apply the configuration changes to the local <code>.pr_agent.toml</code> file in you repository.</p>"},{"location":"pragent/documentation/GENERATE_CUSTOM_LABELS/#3-handle-custom-labels-from-the-repos-labels-page","title":"3. Handle custom labels from the Repo's labels page","text":"<p>This feature is available only in PR-Agent Pro  * GitHub : <code>https://github.com/{owner}/{repo}/labels</code>, or click on the \"Labels\" tab in the issues or PRs page. * GitLab : <code>https://gitlab.com/{owner}/{repo}/-/labels</code>, or click on \"Manage\" -&gt; \"Labels\" on the left menu.</p> <p>b. Add/edit the custom labels. It should be formatted as follows: * Label name: The name of the custom label. * Description: Start the description of with prefix <code>pr_agent:</code>, for example: <code>pr_agent: Description of when AI should suggest this label</code>. The description should be comprehensive and detailed, indicating when to add the desired label. </p> <p>c. Now the custom labels will be included in the <code>generate_labels</code> tool.</p> <p>*This feature is supported in GitHub and GitLab.</p>"},{"location":"pragent/documentation/GENERATE_CUSTOM_LABELS/#configuration-changes","title":"Configuration changes","text":"<ul> <li>Change <code>enable_custom_labels</code> to True: This will turn off the default labels and enable the custom labels provided in the custom_labels.toml file.</li> <li>Add the custom labels. It should be formatted as follows:</li> </ul> <pre><code>[config]\nenable_custom_labels=true\n\n[custom_labels.\"Custom Label Name\"]\ndescription = \"Description of when AI should suggest this label\"\n\n[custom_labels.\"Custom Label 2\"]\ndescription = \"Description of when AI should suggest this label 2\"\n</code></pre>"},{"location":"pragent/documentation/IMPROVE/","title":"Improve Tool","text":""},{"location":"pragent/documentation/IMPROVE/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Configuration options</li> <li>Summarize mode</li> <li>Usage Tips<ul> <li>Extra instructions</li> <li>PR footprint - regular vs summarize mode</li> <li>A note on code suggestions quality</li> </ul> </li> </ul>"},{"location":"pragent/documentation/IMPROVE/#overview","title":"Overview","text":"<p>The <code>improve</code> tool scans the PR code changes, and automatically generates suggestions for improving the PR code. The tool can be triggered automatically every time a new PR is opened, or it can be invoked manually by commenting on any PR: <pre><code>/improve\n</code></pre></p>"},{"location":"pragent/documentation/IMPROVE/#summarized-vs-commitable-code-suggestions","title":"Summarized vs commitable code suggestions","text":"<p>The code suggestions can be presented as a single comment (via <code>pr_code_suggestions.summarize=true</code>):</p> <p></p> <p>Or as a separate commitable code comment for each suggestion:</p> <p></p> <p>Note that a single comment has a significantly smaller PR footprint. We recommend this mode for most cases. Also note that collapsible are not supported in Bitbucket. Hence, the suggestions are presented there as code comments.</p>"},{"location":"pragent/documentation/IMPROVE/#extended-mode","title":"Extended mode","text":"<p>An extended mode, which does not involve PR Compression and provides more comprehensive suggestions, can be invoked by commenting on any PR: <pre><code>/improve --extended\n</code></pre> Note that the extended mode divides the PR code changes into chunks, up to the token limits, where each chunk is handled separately (might use multiple calls to GPT-4 for large PRs). Hence, the total number of suggestions is proportional to the number of chunks, i.e., the size of the PR.</p>"},{"location":"pragent/documentation/IMPROVE/#configuration-options","title":"Configuration options","text":"<p>To edit configurations related to the improve tool (<code>pr_code_suggestions</code> section), use the following template: <pre><code>/improve --pr_code_suggestions.some_config1=... --pr_code_suggestions.some_config2=...\n</code></pre></p>"},{"location":"pragent/documentation/IMPROVE/#general-options","title":"General options","text":"<ul> <li><code>num_code_suggestions</code>: number of code suggestions provided by the 'improve' tool. Default is 4.</li> <li><code>extra_instructions</code>: Optional extra instructions to the tool. For example: \"focus on the changes in the file X. Ignore change in ...\".</li> <li><code>rank_suggestions</code>: if set to true, the tool will rank the suggestions, based on importance. Default is false.</li> <li><code>summarize</code>: if set to true, the tool will display the suggestions in a single comment. Default is false.</li> <li><code>enable_help_text</code>: if set to true, the tool will display a help text in the comment. Default is true.</li> </ul>"},{"location":"pragent/documentation/IMPROVE/#params-for-improve-extended-mode","title":"params for '/improve --extended' mode","text":"<ul> <li><code>auto_extended_mode</code>: enable extended mode automatically (no need for the <code>--extended</code> option). Default is false.</li> <li><code>num_code_suggestions_per_chunk</code>: number of code suggestions provided by the 'improve' tool, per chunk. Default is 8.</li> <li><code>rank_extended_suggestions</code>: if set to true, the tool will rank the suggestions, based on importance. Default is true.</li> <li><code>max_number_of_calls</code>: maximum number of chunks. Default is 5.</li> <li><code>final_clip_factor</code>: factor to remove suggestions with low confidence. Default is 0.9.</li> </ul>"},{"location":"pragent/documentation/IMPROVE/#usage-tips","title":"Usage Tips","text":""},{"location":"pragent/documentation/IMPROVE/#extra-instructions","title":"Extra instructions","text":"<p>Extra instructions are very important for the <code>imrpove</code> tool, since they enable you to guide the model to suggestions that are more relevant to the specific needs of the project.</p> <p>Be specific, clear, and concise in the instructions. With extra instructions, you are the prompter. Specify relevant aspects that you want the model to focus on.</p> <p>Examples for extra instructions: <pre><code>[pr_code_suggestions] # /improve #\nextra_instructions=\"\"\"\nEmphasize the following aspects:\n- Does the code logic cover relevant edge cases?\n- Is the code logic clear and easy to understand?\n- Is the code logic efficient?\n...\n\"\"\"\n</code></pre> Use triple quotes to write multi-line instructions. Use bullet points to make the instructions more readable.</p>"},{"location":"pragent/documentation/IMPROVE/#a-note-on-code-suggestions-quality","title":"A note on code suggestions quality","text":"<ul> <li>While the current AI for code is getting better and better (GPT-4), it's not flawless. Not all the suggestions will be perfect, and a user should not accept all of them automatically.</li> <li>Suggestions are not meant to be simplistic. Instead, they aim to give deep feedback and raise questions, ideas and thoughts to the user, who can then use his judgment, experience, and understanding of the code base.</li> <li>Recommended to use the 'extra_instructions' field to guide the model to suggestions that are more relevant to the specific needs of the project.</li> <li>Best quality will be obtained by using 'improve --extended' mode.</li> </ul>"},{"location":"pragent/documentation/REVIEW/","title":"Review Tool","text":""},{"location":"pragent/documentation/REVIEW/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Configuration options</li> <li>Incremental Mode</li> <li>PR Reflection</li> <li>Usage Tips</li> <li>General guidelines</li> <li>Code suggestions</li> <li>Automation</li> <li>Auto-labels</li> <li>Extra instructions</li> <li>Auto-approval</li> </ul>"},{"location":"pragent/documentation/REVIEW/#overview","title":"Overview","text":"<p>The <code>review</code> tool scans the PR code changes, and automatically generates a PR review. The tool can be triggered automatically every time a new PR is opened, or can be invoked manually by commenting on any PR: <pre><code>/review\n</code></pre> For example:</p> <p></p> <p></p>"},{"location":"pragent/documentation/REVIEW/#configuration-options","title":"Configuration options","text":"<p>To edit configurations  related to the review tool (<code>pr_reviewer</code> section), use the following template: <pre><code>/review --pr_reviewer.some_config1=... --pr_reviewer.some_config2=...\n</code></pre></p>"},{"location":"pragent/documentation/REVIEW/#general-options","title":"General options","text":"<ul> <li><code>num_code_suggestions</code>: number of code suggestions provided by the 'review' tool. Default is 4.</li> <li><code>inline_code_comments</code>: if set to true, the tool will publish the code suggestions as comments on the code diff. Default is false.</li> <li><code>persistent_comment</code>: if set to true, the review comment will be persistent, meaning that every new review request will edit the previous one. Default is true.</li> <li><code>extra_instructions</code>: Optional extra instructions to the tool. For example: \"focus on the changes in the file X. Ignore change in ...\".</li> </ul>"},{"location":"pragent/documentation/REVIEW/#enabledisable-features","title":"Enable\\disable features","text":"<ul> <li><code>require_focused_review</code>: if set to true, the tool will add a section - 'is the PR a focused one'. Default is false.</li> <li><code>require_score_review</code>: if set to true, the tool will add a section that scores the PR. Default is false.</li> <li><code>require_tests_review</code>: if set to true, the tool will add a section that checks if the PR contains tests. Default is true.</li> <li><code>require_security_review</code>: if set to true, the tool will add a section that checks if the PR contains security issues. Default is true.</li> <li><code>require_estimate_effort_to_review</code>: if set to true, the tool will add a section that estimates the effort needed to review the PR. Default is true.</li> </ul>"},{"location":"pragent/documentation/REVIEW/#soc2-ticket-compliance","title":"SOC2 ticket compliance \ud83d\udc8e","text":"<p>This sub-tool checks if the PR description properly contains a ticket to a project management system (e.g., Jira, Asana, Trello, etc.), as required by SOC2 compliance. If not, it will add a label to the PR: \"Missing SOC2 ticket\". - <code>require_soc2_ticket</code>: If set to true, the SOC2 ticket checker sub-tool will be enabled. Default is false. - <code>soc2_ticket_prompt</code>: The prompt for the SOC2 ticket review. Default is: <code>Does the PR description include a link to ticket in a project management system (e.g., Jira, Asana, Trello, etc.) ?</code>. Edit this field if your compliance requirements are different.</p>"},{"location":"pragent/documentation/REVIEW/#adding-pr-labels","title":"Adding PR labels","text":"<ul> <li><code>enable_review_labels_security</code>: if set to true, the tool will publish a 'possible security issue' label if it detects a security issue. Default is true.</li> <li><code>enable_review_labels_effort</code>: if set to true, the tool will publish a 'Review effort [1-5]: x' label. Default is true.</li> </ul>"},{"location":"pragent/documentation/REVIEW/#auto-approval","title":"Auto-approval","text":"<ul> <li><code>enable_auto_approval</code>: if set to true, the tool will approve the PR when invoked with the 'auto_approve' command. Default is false. This flag can be changed only from configuration file.</li> <li><code>maximal_review_effort</code>: maximal effort level for auto-approval. If the PR's estimated review effort is above this threshold, the auto-approval will not run. Default is 5.</li> </ul>"},{"location":"pragent/documentation/REVIEW/#incremental-mode","title":"Incremental Mode","text":"<p>Incremental review only considers changes since the last PR-Agent review. This can be useful when working on the PR in an iterative manner, and you want to focus on the changes since the last review instead of reviewing the entire PR again. For invoking the incremental mode, the following command can be used: <pre><code>/review -i\n</code></pre> Note that the incremental mode is only available for GitHub.</p> <p></p> <p>Under the section 'pr_reviewer', the configuration file contains options to customize the 'review -i' tool. These configurations can be used to control the rate at which the incremental review tool will create new review comments when invoked automatically, to prevent making too much noise in the PR. - <code>minimal_commits_for_incremental_review</code>: Minimal number of commits since the last review that are required to create incremental review. If there are less than the specified number of commits since the last review, the tool will not perform any action. Default is 0 - the tool will always run, no matter how many commits since the last review. - <code>minimal_minutes_for_incremental_review</code>: Minimal number of minutes that need to pass since the last reviewed commit to create incremental review. If less that the specified number of minutes have passed between the last reviewed commit and running this command, the tool will not perform any action.  Default is 0 - the tool will always run, no matter how much time have passed since the last reviewed commit. - <code>require_all_thresholds_for_incremental_review</code>: If set to true, all the previous thresholds must be met for incremental review to run. If false, only one is enough to run the tool. For example, if <code>minimal_commits_for_incremental_review=2</code> and <code>minimal_minutes_for_incremental_review=2</code>, and we have 3 commits since the last review, but the last reviewed commit is from 1 minute ago: When <code>require_all_thresholds_for_incremental_review=true</code> the incremental review will not run, because only 1 out of 2 conditions were met (we have enough commits but the last review is too recent), but when <code>require_all_thresholds_for_incremental_review=false</code> the incremental review will run, because one condition is enough (we have 3 commits which is more than the configured 2). Default is false - the tool will run as long as at least once conditions is met. - <code>remove_previous_review_comment</code>: if set to true, the tool will remove the previous review comment before adding a new one. Default is false.</p>"},{"location":"pragent/documentation/REVIEW/#pr-reflection","title":"PR Reflection","text":"<p>By invoking: <pre><code>/reflect_and_review\n</code></pre> The tool will first ask the author questions about the PR, and will guide the review based on their answers.</p> <p></p> <p></p> <p></p>"},{"location":"pragent/documentation/REVIEW/#usage-tips","title":"Usage Tips","text":"<p>1) General guidelines 2) Code suggestions 3) Automation 4) Auto-labels 5) Extra instructions 6) Auto-approval</p>"},{"location":"pragent/documentation/REVIEW/#general-guidelines","title":"General guidelines","text":"<p>The <code>review</code> tool provides a collection of possible feedbacks about a PR. It is recommended to review the Configuration options section, and choose the relevant options for your use case.</p> <p>Some of the feature that are disabled by default are quite useful, and should be considered for enabling. For example:  <code>require_score_review</code>, <code>require_soc2_ticket</code>, and more.</p> <p>On the other hand, if you find one of the enabled features to be irrelevant for your use case, disable it. No default configuration can fit all use cases.</p>"},{"location":"pragent/documentation/REVIEW/#code-suggestions","title":"Code suggestions","text":"<p>The <code>review</code> tool provides several type of feedbacks, one of them is code suggestions. If you are interested only in the code suggestions, it is recommended to use the <code>improve</code> feature instead, since it dedicated only to code suggestions, and usually gives better results. Use the <code>review</code> tool if you want to get a more comprehensive feedback, which includes code suggestions as well.</p>"},{"location":"pragent/documentation/REVIEW/#automation","title":"Automation","text":"<ul> <li>When you first install the app, the default mode for the <code>review</code> tool is: <pre><code>pr_commands = [\"/review\", ...]\n</code></pre> meaning the <code>review</code> tool will run automatically on every PR, with the default configuration. Edit this field to enable/disable the tool, or to change the used configurations</li> </ul>"},{"location":"pragent/documentation/REVIEW/#auto-labels","title":"Auto-labels","text":"<p>The <code>review</code> tool can auto-generate two specific types of labels for a PR: - a <code>possible security issue</code> label that detects a possible security issue (<code>enable_review_labels_security</code> flag) - a <code>Review effort [1-5]: x</code> label, where x is the estimated effort to review the PR (<code>enable_review_labels_effort</code> flag)</p> <p>Both modes are useful, and we recommended to enable them.</p>"},{"location":"pragent/documentation/REVIEW/#extra-instructions","title":"Extra instructions","text":"<p>Extra instruction are important. The <code>review</code> tool can be configured with extra instructions, which can be used to guide the model to a feedback tailored to the needs of your project.</p> <p>Be specific, clear, and concise in the instructions. With extra instructions, you are the prompter. Specify the relevant sub-tool, and the relevant aspects of the PR that you want to emphasize.</p> <p>Examples for extra instructions: <pre><code>[pr_reviewer] # /review #\nextra_instructions=\"\"\"\nIn the code feedback section, emphasize the following:\n- Does the code logic cover relevant edge cases?\n- Is the code logic clear and easy to understand?\n- Is the code logic efficient?\n...\n\"\"\"\n</code></pre> Use triple quotes to write multi-line instructions. Use bullet points to make the instructions more readable.</p>"},{"location":"pragent/documentation/REVIEW/#auto-approval_1","title":"Auto-approval","text":"<p>PR-Agent can approve a PR when a specific comment is invoked.</p> <p>To ensure safety, the auto-approval feature is disabled by default. To enable auto-approval, you need to actively set in a pre-defined configuration file the following: <pre><code>[pr_reviewer]\nenable_auto_approval = true\n</code></pre> (this specific flag cannot be set with a command line argument, only in the configuration file, committed to the repository)</p> <p>After enabling, by commenting on a PR: <pre><code>/review auto_approve\n</code></pre> PR-Agent will automatically approve the PR, and add a comment with the approval.</p> <p>You can also enable auto-approval only if the PR meets certain requirements, such as that the <code>estimated_review_effort</code> label is equal or below a certain threshold, by adjusting the flag: <pre><code>[pr_reviewer]\nmaximal_review_effort = 5\n</code></pre></p>"},{"location":"pragent/documentation/SIMILAR_ISSUE/","title":"Similar Issue Tool","text":"<p>The similar issue tool retrieves the most similar issues to the current issue. It can be invoked manually by commenting on any PR: <pre><code>/similar_issue\n</code></pre> For example:</p> <p> </p> <p>Note that to perform retrieval, the <code>similar_issue</code> tool indexes all the repo previous issues (once).</p> <p>Select VectorDBs by changing <code>pr_similar_issue</code> parameter in <code>configuration.toml</code> file </p> <p>2 VectorDBs are available to switch in 1. LanceDB 2. Pinecone</p> <p>To enable usage of the 'similar issue' tool for Pinecone, you need to set the following keys in <code>.secrets.toml</code> (or in the relevant environment variables):</p> <p><pre><code>[pinecone]\napi_key = \"...\"\nenvironment = \"...\"\n</code></pre> These parameters can be obtained by registering to Pinecone.</p>"},{"location":"pragent/documentation/SIMILAR_ISSUE/#how-to-use","title":"How to use:","text":"<ul> <li> <p>To invoke the 'similar issue' tool from CLI, run: <code>python3 cli.py --issue_url=... similar_issue</code></p> </li> <li> <p>To invoke the 'similar' issue tool via online usage, comment on a PR: <code>/similar_issue</code></p> </li> <li> <p>You can also enable the 'similar issue' tool to run automatically when a new issue is opened, by adding it to the pr_commands list in the github_app section</p> </li> </ul>"},{"location":"pragent/documentation/TEST/","title":"Test Tool \ud83d\udc8e","text":"<p>By combining LLM abilities with static code analysis, the <code>test</code> tool  generate tests for a selected component, based on the PR code changes. It can be invoked manually by commenting on any PR: <pre><code>/test component_name\n</code></pre> where 'component_name' is the name of a specific component in the PR. To get a list of the components that changed in the PR, use the <code>analyze</code> tool.</p> <p>An example result:</p> <p></p> <p></p> <p></p> <p>Language that are currently supported by the tool: Python, Java, C++, JavaScript, TypeScript.</p>"},{"location":"pragent/documentation/TEST/#configuration-options","title":"Configuration options","text":"<ul> <li><code>num_tests</code>: number of tests to generate. Default is 3.</li> <li><code>testing_framework</code>: the testing framework to use. If not set, for Python it will use <code>pytest</code>, for Java it will use <code>JUnit</code>, for C++ it will use <code>Catch2</code>, and for JavaScript and TypeScript it will use <code>jest</code>.</li> <li><code>avoid_mocks</code>: if set to true, the tool will try to avoid using mocks in the generated tests. Note that even if this option is set to true, the tool might still use mocks if it cannot generate a test without them. Default is true.</li> <li><code>extra_instructions</code>: Optional extra instructions to the tool. For example: \"use the following mock injection scheme: ...\".</li> <li><code>file</code>: in case there are several components with the same name, you can specify the relevant file.</li> <li><code>class_name</code>: in case there are several methods with the same name in the same file, you can specify the relevant class name.</li> <li><code>enable_help_text</code>: if set to true, the tool will add a help text to the PR comment. Default is true.</li> </ul>"},{"location":"pragent/documentation/TOOLS_GUIDE/","title":"TOOLS GUIDE","text":""},{"location":"pragent/documentation/TOOLS_GUIDE/#tools-guide","title":"Tools Guide","text":"<ul> <li>DESCRIBE</li> <li>REVIEW</li> <li>IMPROVE</li> <li>ASK</li> <li>SIMILAR_ISSUE</li> <li>UPDATE CHANGELOG</li> <li>ADD DOCUMENTATION \ud83d\udc8e</li> <li>GENERATE CUSTOM LABELS \ud83d\udc8e</li> <li>Analyze \ud83d\udc8e</li> <li>Test \ud83d\udc8e</li> </ul> <p>See the installation guide for instructions on setting up PR-Agent.</p>"},{"location":"pragent/documentation/UPDATE_CHANGELOG/","title":"Update Changelog Tool","text":"<p>The <code>update_changelog</code> tool automatically updates the CHANGELOG.md file with the PR changes. It can be invoked manually by commenting on any PR: <pre><code>/update_changelog\n</code></pre> For example:</p> <p> </p>"},{"location":"pragent/documentation/UPDATE_CHANGELOG/#configuration-options","title":"Configuration options","text":"<p>Under the section 'pr_update_changelog', the configuration file contains options to customize the 'update changelog' tool:</p> <ul> <li><code>push_changelog_changes</code>: whether to push the changes to CHANGELOG.md, or just print them. Default is false (print only).</li> <li><code>extra_instructions</code>: Optional extra instructions to the tool. For example: \"focus on the changes in the file X. Ignore change in ...</li> </ul>"},{"location":"pragent/gettingstarted/INSTALL/","title":"Installation","text":"<p>To get started with PR-Agent quickly, you first need to acquire two tokens:</p> <ol> <li>An OpenAI key from here, with access to GPT-4.</li> <li>A GitHub\\GitLab\\BitBucket personal access token (classic), with the repo scope. [GitHub from here]</li> </ol> <p>There are several ways to use PR-Agent:</p> <p>Locally - Using Docker image (no installation required) - Run from source</p> <p>GitHub specific methods - Run as a GitHub Action - Run as a polling server - Run as a GitHub App - Deploy as a Lambda Function - AWS CodeCommit</p> <p>GitLab specific methods - Run a GitLab webhook server</p> <p>BitBucket specific methods - Run as a Bitbucket Pipeline - Run on a hosted app - Bitbucket server and data center</p>"},{"location":"pragent/gettingstarted/INSTALL/#use-docker-image-no-installation-required","title":"Use Docker image (no installation required)","text":"<p>A list of the relevant tools can be found in the tools guide.</p> <p>To invoke a tool (for example <code>review</code>), you can run directly from the Docker image. Here's how:</p> <ul> <li> <p>For GitHub: <pre><code>docker run --rm -it -e OPENAI.KEY=&lt;your key&gt; -e GITHUB.USER_TOKEN=&lt;your token&gt; codiumai/pr-agent:latest --pr_url &lt;pr_url&gt; review\n</code></pre></p> </li> <li> <p>For GitLab: <pre><code>docker run --rm -it -e OPENAI.KEY=&lt;your key&gt; -e CONFIG.GIT_PROVIDER=gitlab -e GITLAB.PERSONAL_ACCESS_TOKEN=&lt;your token&gt; codiumai/pr-agent:latest --pr_url &lt;pr_url&gt; review\n</code></pre></p> </li> </ul> <p>Note: If you have a dedicated GitLab instance, you need to specify the custom url as variable: <pre><code>docker run --rm -it -e OPENAI.KEY=&lt;your key&gt; -e CONFIG.GIT_PROVIDER=gitlab -e GITLAB.PERSONAL_ACCESS_TOKEN=&lt;your token&gt; -e GITLAB.URL=&lt;your gitlab instance url&gt; codiumai/pr-agent:latest --pr_url &lt;pr_url&gt; review\n</code></pre></p> <ul> <li>For BitBucket: <pre><code>docker run --rm -it -e CONFIG.GIT_PROVIDER=bitbucket -e OPENAI.KEY=$OPENAI_API_KEY -e BITBUCKET.BEARER_TOKEN=$BITBUCKET_BEARER_TOKEN codiumai/pr-agent:latest --pr_url=&lt;pr_url&gt; review\n</code></pre></li> </ul> <p>For other git providers, update CONFIG.GIT_PROVIDER accordingly, and check the <code>pr_agent/settings/.secrets_template.toml</code> file for the environment variables expected names and values.</p> <p>If you want to ensure you're running a specific version of the Docker image, consider using the image's digest: <pre><code>docker run --rm -it -e OPENAI.KEY=&lt;your key&gt; -e GITHUB.USER_TOKEN=&lt;your token&gt; codiumai/pr-agent@sha256:71b5ee15df59c745d352d84752d01561ba64b6d51327f97d46152f0c58a5f678 --pr_url &lt;pr_url&gt; review\n</code></pre></p> <p>Or you can run a specific released versions of pr-agent, for example: <pre><code>codiumai/pr-agent@v0.9\n</code></pre></p>"},{"location":"pragent/gettingstarted/INSTALL/#run-from-source","title":"Run from source","text":"<ol> <li>Clone this repository:</li> </ol> <pre><code>git clone https://github.com/Codium-ai/pr-agent.git\n</code></pre> <ol> <li>Navigate to the <code>/pr-agent</code> folder and install the requirements in your favorite virtual environment:</li> </ol> <pre><code>pip install -e .\n</code></pre> <p>Note: If you get an error related to Rust in the dependency installation then make sure Rust is installed and in your <code>PATH</code>, instructions: https://rustup.rs</p> <ol> <li>Copy the secrets template file and fill in your OpenAI key and your GitHub user token:</li> </ol> <pre><code>cp pr_agent/settings/.secrets_template.toml pr_agent/settings/.secrets.toml\nchmod 600 pr_agent/settings/.secrets.toml\n# Edit .secrets.toml file\n</code></pre> <ol> <li>Run the cli.py script:</li> </ol> <pre><code>python3 -m pr_agent.cli --pr_url &lt;pr_url&gt; review\npython3 -m pr_agent.cli --pr_url &lt;pr_url&gt; ask &lt;your question&gt;\npython3 -m pr_agent.cli --pr_url &lt;pr_url&gt; describe\npython3 -m pr_agent.cli --pr_url &lt;pr_url&gt; improve\npython3 -m pr_agent.cli --pr_url &lt;pr_url&gt; add_docs\npython3 -m pr_agent.cli --pr_url &lt;pr_url&gt; generate_labels\npython3 -m pr_agent.cli --issue_url &lt;issue_url&gt; similar_issue\n...\n</code></pre> <p>[Optional]\u00a0Add the pr_agent folder to your PYTHONPATH <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;PATH to pr_agent folder&gt;\n</code></pre></p>"},{"location":"pragent/gettingstarted/INSTALL/#run-as-a-github-action","title":"Run as a GitHub Action","text":"<p>You can use our pre-built Github Action Docker image to run PR-Agent as a Github Action.</p> <ol> <li>Add the following file to your repository under <code>.github/workflows/pr_agent.yml</code>:</li> </ol> <p><pre><code>on:\n  pull_request:\n  issue_comment:\njobs:\n  pr_agent_job:\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n      pull-requests: write\n      contents: write\n    name: Run pr agent on every pull request, respond to user comments\n    steps:\n      - name: PR Agent action step\n        id: pragent\n        uses: Codium-ai/pr-agent@main\n        env:\n          OPENAI_KEY: ${{ secrets.OPENAI_KEY }}\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n</code></pre> ** if you want to pin your action to a specific release (v0.7 for example) for stability reasons, use: <pre><code>on:\n  pull_request:\n  issue_comment:\n\njobs:\n  pr_agent_job:\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n      pull-requests: write\n      contents: write\n    name: Run pr agent on every pull request, respond to user comments\n    steps:\n      - name: PR Agent action step\n        id: pragent\n        uses: Codium-ai/pr-agent@v0.7\n        env:\n          OPENAI_KEY: ${{ secrets.OPENAI_KEY }}\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n</code></pre> 2. Add the following secret to your repository under <code>Settings &gt; Secrets and variables &gt; Actions &gt; New repository secret &gt; Add secret</code>:</p> <pre><code>Name = OPENAI_KEY\nSecret = &lt;your key&gt;\n</code></pre> <p>The GITHUB_TOKEN secret is automatically created by GitHub.</p> <ol> <li> <p>Merge this change to your main branch. When you open your next PR, you should see a comment from <code>github-actions</code> bot with a review of your PR, and instructions on how to use the rest of the tools.</p> </li> <li> <p>You may configure PR-Agent by adding environment variables under the env section corresponding to any configurable property in the configuration file. Some examples: <pre><code>      env:\n        # ... previous environment values\n        OPENAI.ORG: \"&lt;Your organization name under your OpenAI account&gt;\"\n        PR_REVIEWER.REQUIRE_TESTS_REVIEW: \"false\" # Disable tests review\n        PR_CODE_SUGGESTIONS.NUM_CODE_SUGGESTIONS: 6 # Increase number of code suggestions\n</code></pre></p> </li> </ol>"},{"location":"pragent/gettingstarted/INSTALL/#run-as-a-polling-server","title":"Run as a polling server","text":"<p>Request reviews by tagging your GitHub user on a PR</p> <p>Follow steps 1-3 of the GitHub Action setup.</p> <p>Run the following command to start the server:</p> <pre><code>python pr_agent/servers/github_polling.py\n</code></pre>"},{"location":"pragent/gettingstarted/INSTALL/#run-as-a-github-app","title":"Run as a GitHub App","text":"<p>Allowing you to automate the review process on your private or public repositories.</p> <ol> <li> <p>Create a GitHub App from the Github Developer Portal.</p> </li> <li> <p>Set the following permissions:</p> <ul> <li>Pull requests: Read &amp; write</li> <li>Issue comment: Read &amp; write</li> <li>Metadata: Read-only</li> <li>Contents: Read-only</li> </ul> </li> <li> <p>Set the following events:</p> <ul> <li>Issue comment</li> <li>Pull request</li> <li>Push (if you need to enable triggering on PR update)</li> </ul> </li> <li> <p>Generate a random secret for your app, and save it for later. For example, you can use:</p> </li> </ol> <pre><code>WEBHOOK_SECRET=$(python -c \"import secrets; print(secrets.token_hex(10))\")\n</code></pre> <ol> <li> <p>Acquire the following pieces of information from your app's settings page:</p> </li> <li> <p>App private key (click \"Generate a private key\" and save the file)</p> </li> <li> <p>App ID</p> </li> <li> <p>Clone this repository:</p> </li> </ol> <pre><code>git clone https://github.com/Codium-ai/pr-agent.git\n</code></pre> <ol> <li>Copy the secrets template file and fill in the following:     <pre><code>cp pr_agent/settings/.secrets_template.toml pr_agent/settings/.secrets.toml\n# Edit .secrets.toml file\n</code></pre></li> <li>Your OpenAI key.</li> <li>Copy your app's private key to the private_key field.</li> <li>Copy your app's ID to the app_id field.</li> <li>Copy your app's webhook secret to the webhook_secret field.</li> <li>Set deployment_type to 'app' in configuration.toml</li> </ol> <p>The .secrets.toml file is not copied to the Docker image by default, and is only used for local development. If you want to use the .secrets.toml file in your Docker image, you can add remove it from the .dockerignore file. In most production environments, you would inject the secrets file as environment variables or as mounted volumes. For example, in order to inject a secrets file as a volume in a Kubernetes environment you can update your pod spec to include the following, assuming you have a secret named <code>pr-agent-settings</code> with a key named <code>.secrets.toml</code>: <pre><code>       volumes:\n        - name: settings-volume\n          secret:\n            secretName: pr-agent-settings\n// ...\n       containers:\n// ...\n          volumeMounts:\n            - mountPath: /app/pr_agent/settings_prod\n              name: settings-volume\n</code></pre></p> <p>Another option is to set the secrets as environment variables in your deployment environment, for example <code>OPENAI.KEY</code> and <code>GITHUB.USER_TOKEN</code>.</p> <ol> <li>Build a Docker image for the app and optionally push it to a Docker repository. We'll use Dockerhub as an example:</li> </ol> <pre><code>docker build . -t codiumai/pr-agent:github_app --target github_app -f docker/Dockerfile\ndocker push codiumai/pr-agent:github_app  # Push to your Docker repository\n</code></pre> <ol> <li> <p>Host the app using a server, serverless function, or container environment. Alternatively, for development and    debugging, you may use tools like smee.io to forward webhooks to your local machine.     You can check Deploy as a Lambda Function</p> </li> <li> <p>Go back to your app's settings, and set the following:</p> </li> <li> <p>Webhook URL: The URL of your app's server or the URL of the smee.io channel.</p> </li> <li> <p>Webhook secret: The secret you generated earlier.</p> </li> <li> <p>Install the app by navigating to the \"Install App\" tab and selecting your desired repositories.</p> </li> </ol> <p>Note: When running PR-Agent from GitHub App, the default configuration file (configuration.toml) will be loaded. However, you can override the default tool parameters by uploading a local configuration file <code>.pr_agent.toml</code> For more information please check out the USAGE GUIDE</p>"},{"location":"pragent/gettingstarted/INSTALL/#deploy-as-a-lambda-function","title":"Deploy as a Lambda Function","text":"<ol> <li>Follow steps 1-5 of Method 5.</li> <li>Build a docker image that can be used as a lambda function     <code>shell     docker buildx build --platform=linux/amd64 . -t codiumai/pr-agent:serverless -f docker/Dockerfile.lambda</code></li> <li>Push image to ECR     <pre><code>docker tag codiumai/pr-agent:serverless &lt;AWS_ACCOUNT&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com/codiumai/pr-agent:serverless\ndocker push &lt;AWS_ACCOUNT&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com/codiumai/pr-agent:serverless\n</code></pre></li> <li>Create a lambda function that uses the uploaded image. Set the lambda timeout to be at least 3m.</li> <li>Configure the lambda function to have a Function URL.</li> <li>In the environment variables of the Lambda function, specify <code>AZURE_DEVOPS_CACHE_DIR</code> to a writable location such as /tmp. (see link)</li> <li>Go back to steps 8-9 of Method 5 with the function url as your Webhook URL.     The Webhook URL would look like <code>https://&lt;LAMBDA_FUNCTION_URL&gt;/api/v1/github_webhooks</code></li> </ol>"},{"location":"pragent/gettingstarted/INSTALL/#aws-codecommit-setup","title":"AWS CodeCommit Setup","text":"<p>Not all features have been added to CodeCommit yet.  As of right now, CodeCommit has been implemented to run the pr-agent CLI on the command line, using AWS credentials stored in environment variables.  (More features will be added in the future.)  The following is a set of instructions to have pr-agent do a review of your CodeCommit pull request from the command line:</p> <ol> <li>Create an IAM user that you will use to read CodeCommit pull requests and post comments<ul> <li>Note: That user should have CLI access only, not Console access</li> </ul> </li> <li>Add IAM permissions to that user, to allow access to CodeCommit (see IAM Role example below)</li> <li>Generate an Access Key for your IAM user</li> <li>Set the Access Key and Secret using environment variables (see Access Key example below)</li> <li>Set the <code>git_provider</code> value to <code>codecommit</code> in the <code>pr_agent/settings/configuration.toml</code> settings file</li> <li>Set the <code>PYTHONPATH</code> to include your <code>pr-agent</code> project directory<ul> <li>Option A: Add <code>PYTHONPATH=\"/PATH/TO/PROJECTS/pr-agent</code> to your <code>.env</code> file</li> <li>Option B: Set <code>PYTHONPATH</code> and run the CLI in one command, for example:<ul> <li><code>PYTHONPATH=\"/PATH/TO/PROJECTS/pr-agent python pr_agent/cli.py [--ARGS]</code></li> </ul> </li> </ul> </li> </ol>"},{"location":"pragent/gettingstarted/INSTALL/#aws-codecommit-iam-role-example","title":"AWS CodeCommit IAM Role Example","text":"<p>Example IAM permissions to that user to allow access to CodeCommit:</p> <ul> <li>Note: The following is a working example of IAM permissions that has read access to the repositories and write access to allow posting comments</li> <li>Note: If you only want pr-agent to review your pull requests, you can tighten the IAM permissions further, however this IAM example will work, and allow the pr-agent to post comments to the PR</li> <li>Note: You may want to replace the <code>\"Resource\": \"*\"</code> with your list of repos, to limit access to only those repos</li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"codecommit:BatchDescribe*\",\n                \"codecommit:BatchGet*\",\n                \"codecommit:Describe*\",\n                \"codecommit:EvaluatePullRequestApprovalRules\",\n                \"codecommit:Get*\",\n                \"codecommit:List*\",\n                \"codecommit:PostComment*\",\n                \"codecommit:PutCommentReaction\",\n                \"codecommit:UpdatePullRequestDescription\",\n                \"codecommit:UpdatePullRequestTitle\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"pragent/gettingstarted/INSTALL/#aws-codecommit-access-key-and-secret","title":"AWS CodeCommit Access Key and Secret","text":"<p>Example setting the Access Key and Secret using environment variables</p> <pre><code>export AWS_ACCESS_KEY_ID=\"XXXXXXXXXXXXXXXX\"\nexport AWS_SECRET_ACCESS_KEY=\"XXXXXXXXXXXXXXXX\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\n</code></pre>"},{"location":"pragent/gettingstarted/INSTALL/#aws-codecommit-cli-example","title":"AWS CodeCommit CLI Example","text":"<p>After you set up AWS CodeCommit using the instructions above, here is an example CLI run that tells pr-agent to review a given pull request. (Replace your specific PYTHONPATH and PR URL in the example)</p> <pre><code>PYTHONPATH=\"/PATH/TO/PROJECTS/pr-agent\" python pr_agent/cli.py \\\n  --pr_url https://us-east-1.console.aws.amazon.com/codesuite/codecommit/repositories/MY_REPO_NAME/pull-requests/321 \\\n  review\n</code></pre>"},{"location":"pragent/gettingstarted/INSTALL/#run-a-gitlab-webhook-server","title":"Run a GitLab webhook server","text":"<ol> <li>From the GitLab workspace or group, create an access token. Enable the \"api\" scope only.</li> <li>Generate a random secret for your app, and save it for later. For example, you can use:</li> </ol> <p><pre><code>WEBHOOK_SECRET=$(python -c \"import secrets; print(secrets.token_hex(10))\")\n</code></pre> 3. Follow the instructions to build the Docker image, setup a secrets file and deploy on your own server from Method 5 steps 4-7. 4. In the secrets file, fill in the following:     - Your OpenAI key.     - In the [gitlab] section, fill in personal_access_token and shared_secret. The access token can be a personal access token, or a group or project access token.     - Set deployment_type to 'gitlab' in configuration.toml 5. Create a webhook in GitLab. Set the URL to the URL of your app's server. Set the secret token to the generated secret from step 2. In the \"Trigger\" section, check the \u2018comments\u2019 and \u2018merge request events\u2019 boxes. 6. Test your installation by opening a merge request or commenting or a merge request using one of CodiumAI's commands.</p>"},{"location":"pragent/gettingstarted/INSTALL/#run-as-a-bitbucket-pipeline","title":"Run as a Bitbucket Pipeline","text":"<p>You can use the Bitbucket Pipeline system to run PR-Agent on every pull request open or update.</p> <ol> <li>Add the following file in your repository bitbucket_pipelines.yml</li> </ol> <pre><code>pipelines:\n    pull-requests:\n      '**':\n        - step:\n            name: PR Agent Review\n            image: python:3.10\n            services:\n              - docker\n            script:\n              - docker run -e CONFIG.GIT_PROVIDER=bitbucket -e OPENAI.KEY=$OPENAI_API_KEY -e BITBUCKET.BEARER_TOKEN=$BITBUCKET_BEARER_TOKEN codiumai/pr-agent:latest --pr_url=https://bitbucket.org/$BITBUCKET_WORKSPACE/$BITBUCKET_REPO_SLUG/pull-requests/$BITBUCKET_PR_ID review\n</code></pre> <ol> <li>Add the following secure variables to your repository under Repository settings &gt; Pipelines &gt; Repository variables. OPENAI_API_KEY: <code>&lt;your key&gt;</code> BITBUCKET_BEARER_TOKEN: <code>&lt;your token&gt;</code></li> </ol> <p>You can get a Bitbucket token for your repository by following Repository Settings -&gt; Security -&gt; Access Tokens.</p> <p>Note that comments on a PR are not supported in Bitbucket Pipeline.</p>"},{"location":"pragent/gettingstarted/INSTALL/#run-using-codiumai-hosted-bitbucket-app","title":"Run using CodiumAI-hosted Bitbucket app","text":"<p>Please contact support@codium.ai or visit CodiumAI pricing page if you're interested in a hosted BitBucket app solution that provides full functionality including PR reviews and comment handling. It's based on the bitbucket_app.py implementation.</p>"},{"location":"pragent/gettingstarted/INSTALL/#bitbucket-server-and-data-center","title":"Bitbucket Server and Data Center","text":"<p>Login into your on-prem instance of Bitbucket with your service account username and password. Navigate to <code>Manage account</code>, <code>HTTP Access tokens</code>, <code>Create Token</code>. Generate the token and add it to .secret.toml under <code>bitbucket_server</code> section</p> <pre><code>[bitbucket_server]\nbearer_token = \"&lt;your key&gt;\"\n</code></pre>"},{"location":"pragent/gettingstarted/INSTALL/#run-it-as-cli","title":"Run it as CLI","text":"<p>Modify <code>configuration.toml</code>:</p> <pre><code>git_provider=\"bitbucket_server\"\n</code></pre> <p>and pass the Pull request URL: <pre><code>python cli.py --pr_url https://git.onpreminstanceofbitbucket.com/projects/PROJECT/repos/REPO/pull-requests/1 review\n</code></pre></p>"},{"location":"pragent/gettingstarted/INSTALL/#run-it-as-service","title":"Run it as service","text":"<p>To run pr-agent as webhook, build the docker image: <pre><code>docker build . -t codiumai/pr-agent:bitbucket_server_webhook --target bitbucket_server_webhook -f docker/Dockerfile\ndocker push codiumai/pr-agent:bitbucket_server_webhook  # Push to your Docker repository\n</code></pre></p> <p>Navigate to <code>Projects</code> or <code>Repositories</code>, <code>Settings</code>, <code>Webhooks</code>, <code>Create Webhook</code>. Fill the name and URL, Authentication None select the Pull Request Opened checkbox to receive that event as webhook.</p> <p>The URL should end with <code>/webhook</code>, for example: https://domain.com/webhook</p> <p>=======</p>"},{"location":"pragent/guides/PR_COMPRESSION/","title":"PR Compression Strategy","text":"<p>There are two scenarios: 1. The PR is small enough to fit in a single prompt (including system and user prompt) 2. The PR is too large to fit in a single prompt (including system and user prompt)</p> <p>For both scenarios, we first use the following strategy</p>"},{"location":"pragent/guides/PR_COMPRESSION/#repo-language-prioritization-strategy","title":"Repo language prioritization strategy","text":"<p>We prioritize the languages of the repo based on the following criteria: 1. Exclude binary files and non code files (e.g. images, pdfs, etc) 2. Given the main languages used in the repo 2. We sort the PR files by the most common languages in the repo (in descending order):     * <code>[[file.py, file2.py],[file3.js, file4.jsx],[readme.md]]</code></p>"},{"location":"pragent/guides/PR_COMPRESSION/#small-pr","title":"Small PR","text":"<p>In this case, we can fit the entire PR in a single prompt: 1. Exclude binary files and non code files (e.g. images, pdfs, etc) 2. We Expand the surrounding context of each patch to 3 lines above and below the patch</p>"},{"location":"pragent/guides/PR_COMPRESSION/#large-pr","title":"Large PR","text":""},{"location":"pragent/guides/PR_COMPRESSION/#motivation","title":"Motivation","text":"<p>Pull Requests can be very long and contain a lot of information with varying degree of relevance to the pr-agent. We want to be able to pack as much information as possible in a single LMM prompt, while keeping the information relevant to the pr-agent.</p>"},{"location":"pragent/guides/PR_COMPRESSION/#compression-strategy","title":"Compression strategy","text":"<p>We prioritize additions over deletions:  - Combine all deleted files into a single list (<code>deleted files</code>)  - File patches are a list of hunks, remove all hunks of type deletion-only from the hunks in the file patch</p>"},{"location":"pragent/guides/PR_COMPRESSION/#adaptive-and-token-aware-file-patch-fitting","title":"Adaptive and token-aware file patch fitting","text":"<p>We use tiktoken to tokenize the patches after the modifications described above, and we use the following strategy to fit the patches into the prompt: 1. Within each language we sort the files by the number of tokens in the file (in descending order):    * <code>[[file2.py, file.py],[file4.jsx, file3.js],[readme.md]]</code> 2. Iterate through the patches in the order described above 2. Add the patches to the prompt until the prompt reaches a certain buffer from the max token length 3. If there are still patches left, add the remaining patches as a list called <code>other modified files</code> to the prompt until the prompt reaches the max token length (hard stop), skip the rest of the patches. 4. If we haven't reached the max token length, add the <code>deleted files</code> to the prompt until the prompt reaches the max token length (hard stop), skip the rest of the patches.</p>"},{"location":"pragent/guides/PR_COMPRESSION/#example","title":"Example","text":""}]}